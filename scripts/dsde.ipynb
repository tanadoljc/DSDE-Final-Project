{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uABJZywV-znq"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es1tXLiQhRVL",
        "outputId": "95b98b83-c0a0-4261-e6a5-b9e1cbd1b83d"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init(\"/Users/4most/Inclass/DSDE/final/spark-3.5.5-bin-hadoop3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_YSBajaPhSnb"
      },
      "outputs": [],
      "source": [
        "spark_url = 'local'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rnPQ51DUhSsY"
      },
      "outputs": [],
      "source": [
        "#library\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, substring, to_date, weekofyear, explode, regexp_replace, split, expr, min, round, date_sub, avg\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from pyspark.sql import functions as F\n",
        "import math\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lR4R451rhSu_"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .master(spark_url) \\\n",
        "    .appName('Spark SQL') \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dshz-VGUx-sy"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KaG11FEnhSxV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "path = '../bangkok_traffy.csv'\n",
        "df = spark.read.csv(path, header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raAKtyY7hSz7",
        "outputId": "e57b9832-db32-43fd-d8d3-01b2bb5eebb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- ticket_id: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- organization: string (nullable = true)\n",
            " |-- comment: string (nullable = true)\n",
            " |-- coords: string (nullable = true)\n",
            " |-- address: string (nullable = true)\n",
            " |-- subdistrict: string (nullable = true)\n",
            " |-- district: string (nullable = true)\n",
            " |-- province: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- star: string (nullable = true)\n",
            " |-- count_reopen: string (nullable = true)\n",
            " |-- last_activity: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#deleted column\n",
        "del_col = ['photo','photo_after']\n",
        "\n",
        "df = df.drop(*del_col)\n",
        "\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOgCPdRokHPp",
        "outputId": "169d3bf0-172d-49c8-8430-9cea26dbe805"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1718441"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_LvHy1NqhS2A"
      },
      "outputs": [],
      "source": [
        "# at least 5 non-null values\n",
        "df = df.na.drop(thresh=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wcvuFCM3hS4n"
      },
      "outputs": [],
      "source": [
        "# deselect the unknown type\n",
        "df = df.filter(df[\"type\"] != \"{}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zZzIsQfPhS69"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn(\"date\", to_date(substring('timestamp',1,10), \"yyyy-MM-dd\"))\n",
        "df = df.withColumn(\"year\", substring(\"date\", 1, 4))\n",
        "df = df.withColumn(\"week\", weekofyear(\"date\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6mh2I2HQ--7a"
      },
      "outputs": [],
      "source": [
        "# df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oYNFqDS0hS9n"
      },
      "outputs": [],
      "source": [
        "# filter more\n",
        "df = df.filter(col(\"date\") > \"2022-09-01\")\n",
        "df = df.filter(col('subdistrict').isNotNull() & col('district').isNotNull() & col('province').isNotNull())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIxt_ujVkJjy",
        "outputId": "e0198779-f6c5-46a0-c790-2e5e837e75ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "384906"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BMc3X5NhiQrg"
      },
      "outputs": [],
      "source": [
        "# filter the wrong case\n",
        "train_col = ['type', 'coords', 'subdistrict', 'district', 'province', 'timestamp']\n",
        "\n",
        "for c in train_col:\n",
        "  df = df.filter(~(col(c).startswith(\"https:\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Yez0PlbNirLX"
      },
      "outputs": [],
      "source": [
        "# df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "W78cYxsRhh6B"
      },
      "outputs": [],
      "source": [
        "df_cleaned = df.withColumn(\"type\", regexp_replace(col(\"type\"), \"[{}]\", \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "D-xErF4yju2G"
      },
      "outputs": [],
      "source": [
        "allowed_types = ['ป้ายจราจร', 'เสียงรบกวน', 'สะพาน', 'PM2.5', 'กีดขวาง', 'ห้องน้ำ', 'ทางเท้า', 'ต้นไม้', 'การเดินทาง', 'คนจรจัด', 'ป้าย', 'ความสะอาด', 'คลอง', 'แสงสว่าง', 'น้ำท่วม', 'ท่อระบายน้ำ', 'ถนน', 'จราจร', 'ความปลอดภัย', 'สายไฟ', 'สัตว์จรจัด']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TfudDbqBi8se"
      },
      "outputs": [],
      "source": [
        "# สร้าง array literal สำหรับ allowed_types\n",
        "allowed_types_expr = \"array({})\".format(\",\".join([f\"'{t}'\" for t in allowed_types]))\n",
        "\n",
        "# สร้าง DataFrame ใหม่โดยแยก field 'type' เป็น array\n",
        "df_with_array = df_cleaned.withColumn(\"type_array\", split(\"type\", \",\"))\n",
        "\n",
        "# ใช้ Spark SQL expression เพื่อตรวจว่า type_array ⊆ allowed_types\n",
        "filtered_df = df_with_array.filter(\n",
        "    expr(f\"array_except(type_array, {allowed_types_expr}) = array()\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Snst4J2Lz2qp"
      },
      "outputs": [],
      "source": [
        "# fix lat long already\n",
        "filtered_df = filtered_df.withColumn(\"lat\", split(col(\"coords\"), \",\")[1]) \\\n",
        "       .withColumn(\"long\", split(col(\"coords\"), \",\")[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3JY_-n8218j",
        "outputId": "20956e26-dd5e-4cd6-8c89-5676ab42aec3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "359557"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for field in filtered_df.schema.fields:\n",
        "#     if str(field.dataType).startswith(\"ArrayType\"):\n",
        "#         filtered_df = filtered_df.withColumn(field.name, col(field.name).cast(\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filtered_df.coalesce(1).write.csv('used_data', header=True, mode='overwrite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjedEuTCx1tx"
      },
      "source": [
        "# Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "s_qhbrXvwOWG"
      },
      "outputs": [],
      "source": [
        "trained_df = filtered_df.select('year', 'week', 'date', 'lat', 'long', 'subdistrict', 'district', 'province', 'type_array')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewu5xCecwqFW",
        "outputId": "4606a758-3651-4245-fc72-7f432313ac6e"
      },
      "outputs": [],
      "source": [
        "# trained_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyWB77FH0d-0"
      },
      "source": [
        "Multi-Hot Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fs31dfXt1i1D"
      },
      "outputs": [],
      "source": [
        "# from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# cv = CountVectorizer(inputCol=\"type_array\", outputCol=\"type_vector\", binary=True)\n",
        "# model = cv.fit(trained_df)\n",
        "# df_vectorized = model.transform(trained_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "kmMolYVjWJJA"
      },
      "outputs": [],
      "source": [
        "# df_vectorized.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1OxDtt5J1qyJ"
      },
      "outputs": [],
      "source": [
        "# trained_df.orderBy(desc('date')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDSJ-JQo13Oz",
        "outputId": "b2823b59-4026-4536-abf2-e18f7545d2e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "359557"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trained_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WwXvX69YtoH"
      },
      "source": [
        "add holiday data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yB9bfMGZYtaU"
      },
      "outputs": [],
      "source": [
        "holiday_path = '../external_csv/bangkok_holidays.csv'\n",
        "holiday_df = spark.read.csv(holiday_path, header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwlxXNbYY8Vk",
        "outputId": "1cd2e556-7792-45d3-e21a-a094d8d22bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+--------------------+\n",
            "|holiday_date|        holiday_name|\n",
            "+------------+--------------------+\n",
            "|  2021-01-01|      New Year's Day|\n",
            "|  2021-02-12|    Chinese New Year|\n",
            "|  2021-02-26|         Makha Bucha|\n",
            "|  2021-04-06|          Chakri Day|\n",
            "|  2021-04-12|    Songkran Holiday|\n",
            "|  2021-04-13|            Songkran|\n",
            "|  2021-04-14|    Songkran Holiday|\n",
            "|  2021-04-15|    Songkran Holiday|\n",
            "|  2021-05-03|Labour Day (in lieu)|\n",
            "|  2021-05-04|H.M. King's Coron...|\n",
            "|  2021-05-10|Royal Ploughing C...|\n",
            "|  2021-05-26|   Visakha Bucha Day|\n",
            "|  2021-06-03|H.M. Queen's Birt...|\n",
            "|  2021-07-25|       Buddhist Lent|\n",
            "|  2021-07-26|Asahna Bucha Day ...|\n",
            "|  2021-07-28|H.M. King's Birthday|\n",
            "|  2021-08-12|        Mother's Day|\n",
            "|  2021-08-12|H.M. Queen Mother...|\n",
            "|  2021-09-24|  Prince Mahidol Day|\n",
            "|  2021-10-13|The Passing of Ki...|\n",
            "+------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# holiday_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "F5bvc7XMaRPa"
      },
      "outputs": [],
      "source": [
        "trained_df = trained_df.withColumn(\"date\", F.to_date(\"date\"))\n",
        "holiday_df = holiday_df.withColumn(\"holiday_date\", F.to_date(\"holiday_date\"))\n",
        "\n",
        "# Extract week start (Monday) for main and holiday dates\n",
        "trained_df = trained_df.withColumn(\"week_start\", F.date_sub(\"date\", F.dayofweek(\"date\") - 2))\n",
        "holiday_df = holiday_df.withColumn(\"holiday_week_start\", F.date_sub(\"holiday_date\", F.dayofweek(\"holiday_date\") - 2))\n",
        "\n",
        "# Join on the same week_start\n",
        "df_joined = trained_df.join(holiday_df, trained_df.week_start == holiday_df.holiday_week_start, how=\"left\")\n",
        "\n",
        "# If holiday_date is not null, it means that week has a holiday\n",
        "df_res = df_joined.withColumn(\"holiday_this_week\", F.when(F.col(\"holiday_date\").isNotNull(), True).otherwise(False))\n",
        "\n",
        "df_res = df_res.drop(\"holiday_name\", \"holiday_week_start\", \"holiday_date\", \"week_start\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-3e2YrtcftC",
        "outputId": "8fc19008-36a0-40f0-aa05-f5360e6b03de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 21:=============================>                            (2 + 1) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+----------+--------+---------+--------------+-----------+-------------+--------------------+-----------------+\n",
            "|year|week|      date|     lat|     long|   subdistrict|   district|     province|          type_array|holiday_this_week|\n",
            "+----+----+----------+--------+---------+--------------+-----------+-------------+--------------------+-----------------+\n",
            "|2022|  37|2022-09-18|13.77422|100.53592|      สามเสนใน|      พญาไท|กรุงเทพมหานคร|       [ท่อระบายน้ำ]|             true|\n",
            "|2022|  37|2022-09-18|13.71498|100.58540|       พระโขนง|    คลองเตย|กรุงเทพมหานคร|        [เสียงรบกวน]|             true|\n",
            "|2022|  37|2022-09-18|13.78256|100.68249|      สะพานสูง|   สะพานสูง|กรุงเทพมหานคร|           [ทางเท้า]|             true|\n",
            "|2022|  37|2022-09-18|13.74363|100.49609|วังบูรพาภิรมย์|     พระนคร|กรุงเทพมหานคร|           [กีดขวาง]|             true|\n",
            "|2022|  37|2022-09-18|13.72409|100.55125|       ลุมพินี|    ปทุมวัน|กรุงเทพมหานคร|       [ท่อระบายน้ำ]|             true|\n",
            "|2022|  37|2022-09-18|13.72398|100.55153|       ลุมพินี|    ปทุมวัน|กรุงเทพมหานคร|             [สายไฟ]|             true|\n",
            "|2022|  37|2022-09-18|13.72381|100.55155|       ลุมพินี|    ปทุมวัน|กรุงเทพมหานคร|       [ท่อระบายน้ำ]|             true|\n",
            "|2022|  37|2022-09-18|13.72393|100.55156|       ลุมพินี|    ปทุมวัน|กรุงเทพมหานคร|         [ความสะอาด]|             true|\n",
            "|2022|  37|2022-09-18|13.72481|100.55198|       ลุมพินี|    ปทุมวัน|กรุงเทพมหานคร|       [ท่อระบายน้ำ]|             true|\n",
            "|2022|  37|2022-09-18|13.88145|100.57549|   ทุ่งสองห้อง|    หลักสี่|กรุงเทพมหานคร|[ทางเท้า, ความสะอาด]|             true|\n",
            "|2022|  37|2022-09-18|13.72473|100.55160|       ลุมพินี|    ปทุมวัน|กรุงเทพมหานคร|[ความสะอาด, ท่อระ...|             true|\n",
            "|2022|  37|2022-09-18|13.88141|100.57697|   ทุ่งสองห้อง|    หลักสี่|กรุงเทพมหานคร|           [ทางเท้า]|             true|\n",
            "|2022|  37|2022-09-18|13.72564|100.55129|       ลุมพินี|    ปทุมวัน|กรุงเทพมหานคร|         [ความสะอาด]|             true|\n",
            "|2022|  37|2022-09-18|13.62752|100.44162|       ท่าข้าม|บางขุนเทียน|กรุงเทพมหานคร|             [สะพาน]|             true|\n",
            "|2022|  37|2022-09-18|13.74058|100.57504|  คลองตันเหนือ|      วัฒนา|กรุงเทพมหานคร|         [ถนน, คลอง]|             true|\n",
            "|2022|  37|2022-09-18|13.72864|100.55021|       ลุมพินี|    ปทุมวัน|กรุงเทพมหานคร|             [สายไฟ]|             true|\n",
            "|2022|  37|2022-09-18|13.68635|100.38235|       หนองแขม|    หนองแขม|กรุงเทพมหานคร|               [ถนน]|             true|\n",
            "|2022|  37|2022-09-18|13.72879|100.55028|       ลุมพินี|    ปทุมวัน|กรุงเทพมหานคร|             [สายไฟ]|             true|\n",
            "|2022|  37|2022-09-18|13.77025|100.57340|      ห้วยขวาง|   ห้วยขวาง|กรุงเทพมหานคร|  [ถนน, ความปลอดภัย]|             true|\n",
            "|2022|  37|2022-09-18|13.72705|100.52232|      สี่พระยา|     บางรัก|กรุงเทพมหานคร|           [ทางเท้า]|             true|\n",
            "+----+----+----------+--------+---------+--------------+-----------+-------------+--------------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# df_res.filter(df_res['holiday_this_week'] == 'true').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYzYarKuPuFy"
      },
      "source": [
        "add rain data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Ev90fK1DaUBN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "\n",
        "df_res = df_res.withColumn(\"date\", to_date(col(\"date\")))\n",
        "# df_res = df_res.filter(col(\"date\") > \"2022-09-01\")\n",
        "\n",
        "df_res = df_res.drop(\"rounded_lat\", \"rounded_lon\")\n",
        "\n",
        "df_res = df_res.withColumn(\"rounded_lat\", round(col(\"lat\"), 2)) \\\n",
        "                             .withColumn(\"rounded_lon\", round(col(\"long\"), 2))\n",
        "\n",
        "# fix lat long already\n",
        "rain_df = spark.read.csv(\"../external_csv/rainfall_data.csv\", header=True, inferSchema=True)\n",
        "rain_df = rain_df.withColumn(\"date\", to_date(col(\"date\"))) \\\n",
        "                 .withColumn(\"rounded_lat\", round(col(\"long\"), 2)) \\\n",
        "                 .withColumn(\"rounded_lon\", round(col(\"lat\"), 2)) \\\n",
        "                 .select(\"date\", \"rounded_lat\", \"rounded_lon\", \"Precipitation (mm)\")\n",
        "\n",
        "df_res = df_res.withColumn(\"date_start\", date_sub(col(\"date\"), 7)) \\\n",
        "                             .withColumn(\"date_end\", col(\"date\"))\n",
        "\n",
        "rain_df_renamed = rain_df.withColumnRenamed(\"date\", \"rain_date\")\n",
        "df_merged = df_res.join(\n",
        "    rain_df_renamed,\n",
        "    (df_res.rounded_lat == rain_df_renamed.rounded_lat) &\n",
        "    (df_res.rounded_lon == rain_df_renamed.rounded_lon) &\n",
        "    (rain_df_renamed.rain_date >= df_res.date_start) &\n",
        "    (rain_df_renamed.rain_date <= df_res.date_end),\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "\n",
        "group_by_columns = [df_res[c] for c in df_res.columns if c not in [\"date_start\", \"date_end\"]]\n",
        "df_merged = df_merged.select(\n",
        "    *group_by_columns,\n",
        "    col(\"Precipitation (mm)\")\n",
        ").groupBy(\n",
        "    *[df_res[c] for c in df_res.columns if c not in [\"date_start\", \"date_end\"]]\n",
        ").agg(\n",
        "    avg(col(\"Precipitation (mm)\")).alias(\"avg_precipitation_7days\")\n",
        ")\n",
        "\n",
        "df_merged = df_merged.fillna({\"avg_precipitation_7days\": 0})\n",
        "\n",
        "df_merged = df_merged.withColumn(\"lat\", col(\"lat\")) \\\n",
        "                     .withColumn(\"long\", col(\"long\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTZyKu28uk18",
        "outputId": "cb9106c9-00de-4ed0-e6cb-5dfdd3b238c1"
      },
      "outputs": [],
      "source": [
        "# df_merged.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFc0nzfJbmkP"
      },
      "source": [
        "add pm2.5 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "jXeUSbak1Ur2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# === Step 1: Load PM2.5 data ===\n",
        "pm25_df = spark.read.csv(\"../external_csv/pm25_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Format columns\n",
        "pm25_df = pm25_df.withColumn(\"date\", to_date(col(\"date\"))) \\\n",
        "                 .withColumn(\"rounded_lat\", round(col(\"rounded_lat\"), 2)) \\\n",
        "                 .withColumn(\"rounded_lon\", round(col(\"rounded_lon\"), 2)) \\\n",
        "                 .select(\"date\", \"rounded_lat\", \"rounded_lon\", \"pm2_5\")\n",
        "\n",
        "# === Step 3: Join with PM2.5 ===\n",
        "df_final = df_merged.join(\n",
        "    pm25_df,\n",
        "    on=[\n",
        "        df_merged.date == pm25_df.date,\n",
        "        df_merged.rounded_lat == pm25_df.rounded_lat,\n",
        "        df_merged.rounded_lon == pm25_df.rounded_lon\n",
        "    ],\n",
        "    how=\"inner\"\n",
        ").select(\n",
        "    df_merged[\"*\"],  # all columns from df_merged\n",
        "    pm25_df[\"pm2_5\"] # only the PM2.5 value from the right table\n",
        ")\n",
        "\n",
        "df_final = df_final.fillna({\"pm2_5\": 0.0})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcAIl5em1i6A",
        "outputId": "010e446b-bbb7-41d2-97f3-21c44fb73087"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 54:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+----------+--------+---------+-----------+-----------+-------------+--------------------+-----------------+-----------+-----------+-----------------------+-----------------+\n",
            "|year|week|      date|     lat|     long|subdistrict|   district|     province|          type_array|holiday_this_week|rounded_lat|rounded_lon|avg_precipitation_7days|            pm2_5|\n",
            "+----+----+----------+--------+---------+-----------+-----------+-------------+--------------------+-----------------+-----------+-----------+-----------------------+-----------------+\n",
            "|2022|  35|2022-09-02|13.59606|100.40091|      แสมดำ|บางขุนเทียน|กรุงเทพมหานคร|         [ความสะอาด]|            false|       13.6|      100.4|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.60218|100.42565|      แสมดำ|บางขุนเทียน|กรุงเทพมหานคร|           [กีดขวาง]|            false|       13.6|     100.43|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.60674|100.43503|    ท่าข้าม|บางขุนเทียน|กรุงเทพมหานคร|[ถนน, ท่อระบายน้ำ...|            false|      13.61|     100.44|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.61484|100.45025|    ท่าข้าม|บางขุนเทียน|กรุงเทพมหานคร|          [แสงสว่าง]|            false|      13.61|     100.45|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.62320|100.51135|    ทุ่งครุ|    ทุ่งครุ|กรุงเทพมหานคร|            [ต้นไม้]|            false|      13.62|     100.51|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.63379|100.37372|  บางบอนใต้|     บางบอน|กรุงเทพมหานคร|          [แสงสว่าง]|            false|      13.63|     100.37|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.62686|100.41459|      แสมดำ|บางขุนเทียน|กรุงเทพมหานคร|        [เสียงรบกวน]|            false|      13.63|     100.41|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.63434|100.42234|      แสมดำ|บางขุนเทียน|กรุงเทพมหานคร|           [ทางเท้า]|            false|      13.63|     100.42|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.63325|100.42698|      แสมดำ|บางขุนเทียน|กรุงเทพมหานคร|           [ทางเท้า]|            false|      13.63|     100.43|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.63351|100.42637|      แสมดำ|บางขุนเทียน|กรุงเทพมหานคร|           [ทางเท้า]|            false|      13.63|     100.43|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.62684|100.44228|    ท่าข้าม|บางขุนเทียน|กรุงเทพมหานคร|          [แสงสว่าง]|            false|      13.63|     100.44|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.62680|100.44415|    ท่าข้าม|บางขุนเทียน|กรุงเทพมหานคร|          [แสงสว่าง]|            false|      13.63|     100.44|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.63355|100.51519|    ทุ่งครุ|    ทุ่งครุ|กรุงเทพมหานคร|          [แสงสว่าง]|            false|      13.63|     100.52|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.63740|100.41837|      แสมดำ|บางขุนเทียน|กรุงเทพมหานคร|      [ถนน, ทางเท้า]|            false|      13.64|     100.42|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.65484|100.47562|      บางมด|    ทุ่งครุ|กรุงเทพมหานคร|        [เสียงรบกวน]|            false|      13.65|     100.48|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.66235|100.42597|      แสมดำ|บางขุนเทียน|กรุงเทพมหานคร|               [ถนน]|            false|      13.66|     100.43|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.66157|100.42556|      แสมดำ|บางขุนเทียน|กรุงเทพมหานคร|               [ถนน]|            false|      13.66|     100.43|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.66308|100.61217|   บางนาใต้|      บางนา|กรุงเทพมหานคร|             [สายไฟ]|            false|      13.66|     100.61|                   6.32|51.92083333333333|\n",
            "|2022|  35|2022-09-02|13.66549|100.40598|คลองบางพราน|     บางบอน|กรุงเทพมหานคร|        [ถนน, จราจร]|            false|      13.67|     100.41|                   6.32|66.65833333333333|\n",
            "|2022|  35|2022-09-02|13.66754|100.41330|คลองบางพราน|     บางบอน|กรุงเทพมหานคร|          [แสงสว่าง]|            false|      13.67|     100.41|                   6.32|66.65833333333333|\n",
            "+----+----+----------+--------+---------+-----------+-----------+-------------+--------------------+-----------------+-----------+-----------+-----------------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# df_final.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "K0VXnMEd1yPg"
      },
      "outputs": [],
      "source": [
        "df_final = df_final.withColumn(\"avg_precipitation_7days\", round(col(\"avg_precipitation_7days\"), 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "8PHojMrn2GvO"
      },
      "outputs": [],
      "source": [
        "df_final = df_final.withColumn(\"pm2_5\", round(col(\"pm2_5\"), 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "KUg_8fkj2UZL"
      },
      "outputs": [],
      "source": [
        "df_used = df_final.select('year', 'week', 'date', 'lat', 'long', 'subdistrict', 'district', 'province', 'type_array', 'holiday_this_week', 'avg_precipitation_7days', 'pm2_5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRBiLS6G81eC",
        "outputId": "9abad5cb-0091-4311-e2ff-ae8f34a20617"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "259248"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_used.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "FwiKU_hP9eaw"
      },
      "outputs": [],
      "source": [
        "for field in df_used.schema.fields:\n",
        "    if str(field.dataType).startswith(\"ArrayType\"):\n",
        "        df_used = df_used.withColumn(field.name, col(field.name).cast(\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "hjVYfNYb9IB-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_used.coalesce(1).write.csv('used_output', header=True, mode='overwrite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIUEi4Y7rPiL"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# import pandas as pd\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Assuming df_merged is a Spark DataFrame\n",
        "# unique_coords_df = df_merged.select(\"rounded_lat\", \"rounded_lon\").dropDuplicates()\n",
        "# unique_coords = unique_coords_df.toPandas()  # Convert to Pandas for API loop\n",
        "\n",
        "# print(f\"Unique coordinates: {len(unique_coords)}\")\n",
        "\n",
        "# start = \"2022-09-02\"\n",
        "# end = \"2025-01-16\"\n",
        "# all_results = []\n",
        "\n",
        "# for _, row in unique_coords.iterrows():\n",
        "#     lat = row['rounded_lat']\n",
        "#     lon = row['rounded_lon']\n",
        "\n",
        "#     print(f\"📡 Fetching PM2.5 for ({lat}, {lon})...\")\n",
        "\n",
        "#     try:\n",
        "#         url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
        "#         params = {\n",
        "#             \"latitude\": lat,\n",
        "#             \"longitude\": lon,\n",
        "#             \"hourly\": \"pm2_5\",\n",
        "#             \"start_date\": start,\n",
        "#             \"end_date\": end,\n",
        "#             \"timezone\": \"Asia/Bangkok\"\n",
        "#         }\n",
        "\n",
        "#         response = requests.get(url, params=params)\n",
        "#         data = response.json()\n",
        "\n",
        "#         hourly_data = data.get(\"hourly\", {})\n",
        "#         timestamps = hourly_data.get(\"time\", [])\n",
        "#         values = hourly_data.get(\"pm2_5\", [])\n",
        "\n",
        "#         if not timestamps or not values:\n",
        "#             print(f\"❌ No data for ({lat}, {lon})\")\n",
        "#             continue\n",
        "\n",
        "#         pm25_df = pd.DataFrame({\n",
        "#             \"datetime\": pd.to_datetime(timestamps),\n",
        "#             \"pm2_5\": values\n",
        "#         })\n",
        "\n",
        "#         # 🗓️ Extract date and average per day\n",
        "#         pm25_df['date'] = pm25_df['datetime'].dt.date\n",
        "#         pm25_df = pm25_df.groupby('date', as_index=False).mean(numeric_only=True)\n",
        "#         pm25_df['rounded_lat'] = lat\n",
        "#         pm25_df['rounded_lon'] = lon\n",
        "\n",
        "#         all_results.append(pm25_df)\n",
        "\n",
        "#         print(f\"✅ Success: {len(pm25_df)} daily values for ({lat}, {lon})\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Error fetching for ({lat}, {lon}): {e}\")\n",
        "\n",
        "# # 💾 Save all results\n",
        "# if all_results:\n",
        "#     result_df = pd.concat(all_results, ignore_index=True)\n",
        "#     result_df.to_csv(\"pm25_data.csv\", index=False)\n",
        "#     print(\"✅ All results saved to pm25_data.csv\")\n",
        "# else:\n",
        "#     print(\"❌ No valid data to save.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
